---
title: "Exploratory"
author: "Datafeellings"
date: "July 25, 2015"
output: html_document
---

Setup project workspace

```{r, cache=TRUE, echo=FALSE}

ifelse(!dir.exists(file.path("~/Coursera/Capstone/Source")),dir.create(file.path("~/Coursera/Capstone/Source")), FALSE)
ifelse(!dir.exists(file.path("~/Coursera/Capstone/Code")),dir.create(file.path("~/Coursera/Capstone/Code")), FALSE)
ifelse(!dir.exists(file.path("~/Coursera/Capstone/Figures")),dir.create(file.path("~/Coursera/Capstone/Figures")), FALSE)
ifelse(!dir.exists(file.path("~/Coursera/Capstone/Processed_data")),dir.create(file.path("~/Coursera/Capstone/Processed_data")), FALSE)
setwd("~/Coursera/Capstone")
```

Download the data

```{r}

# if (!file.exists("Coursera-SwiftKey.zip")) 
# {
#   download.file(url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile = "~/Documents/Disk D/Coursera/Capstone/Source/Coursera-SwiftKey.zip", method = "curl")
#   system("unzip Coursera-SwiftKey.zip")
# }


```

Load the data into the WS using readLines() function

```{r}

con1 <- file("~/Coursera/Capstone/Source/final/en_US/en_US.blogs.txt", "r")
en_b <- readLines(con1) 
close(con1)

con2 <- file("~/Coursera/Capstone/Source/final/en_US/en_US.news.txt", "r")
en_n <- readLines(con2) 
close(con2)

con3 <- file("~/Coursera/Capstone/Source/final/en_US/en_US.twitter.txt", "r")
en_t <- readLines(con3) 
close(con3)

```

Convert the read corpora into data tables and take a 3000 lines sample of each. Save the samples, as we will work with them for tokenization. For reproducibility, note the value of the set.seed() function.

```{r}

require(data.table) # load the required package
set.seed(888)

en_b = as.data.table(en_b) 
en_b_samp = en_b [sample(.N,3000)] # random sample of 3k lines
write.csv(en_b_samp, file = "Processed_data/en_b_samp.csv") #write the sample

en_n = as.data.table(en_n) 
en_n_samp = en_n [sample(.N,3000)] # random sample of 3k lines
write.csv(en_n_samp, file = "Processed_data/en_n_samp.csv")

en_t = as.data.table(en_t) 
en_t_samp = en_t[sample(.N,3000)] # random sample of 3k lines
write.csv(en_t_samp, file = "Processed_data/en_t_samp.csv")
```

Now we read in the samples using the tm package as a single corpus. As the goal is to tokenize the words and remove profanity, no differentiation is required between the different sources of the text.

```{r}
setwd("Coursera/Capstone/")
require("tm")
require("SnowballC")
samp = Corpus(DirSource("Processed_data"), readerControl = list(language = "en_EN", load = F))

```

Removing profanity


```{r}
# source of the English language profanity list used by Google
# credit goes to https://gist.github.com/jamiew

con4 = url(description = "https://gist.githubusercontent.com/jamiew/1112488/raw/7ca9b1669e1c24b27c66174762cb04e14cf05aa7/google_twunter_lol")
profanity = readLines(con4)
close(con4)

dir.create(path = "Source/external")
write.csv(x = profanity, file = "Source/external/profanity.csv")
profanity1 = read.csv("Source/external/profanity.csv", skip = 1,nrows = 451)

# clean the file to turn it into a character string that can be used by tm 
profanity1=profanity1[,-1]
profanity1=as.character(profanity1)

profanity1=gsub("\"", x= profanity1, replacement ="")
profanity1=gsub(pattern =":1", x= profanity1, replacement ="")
profanity1=gsub(pattern =",", x= profanity1, replacement ="")
profanity1 = tolower(profanity1)
rm(profanity)

```

Preprocessing the text using the automated functions of the "tm" package.

```{r}
# Input for this part for the workflow: https://gist.github.com/dsparks/4260167

# Make lowercase & remove punctuation
samp_clean = tm_map(samp, tolower)

# Remove special characters and punctuation
samp_clean = tm_map(samp_clean, removePunctuation)

for(j in seq(samp_clean))   
{   
  samp_clean[[j]] = iconv(samp_clean[[j]], to="ASCII", sub = "") 
  samp_clean[[j]] <- gsub("/", " ", samp_clean[[j]])   
    samp_clean[[j]] <- gsub("@", " ", samp_clean[[j]])   
    samp_clean[[j]] <- gsub("\\|", " ", samp_clean[[j]])
#    samp_clean[[j]] <- gsub("â€", " ", samp_clean[[j]])
#    samp_clean[[j]] <- gsub("^[^:alnum:]", " ", samp_clean[[j]])
}  
samp_clean = tm_map(samp_clean, PlainTextDocument)  

samp_clean = tm_map(samp_clean, removeNumbers)

# Remove profanity
#samp_clean = tm_map(samp_clean, removeWords, stopwords("english")) 
#samp_clean = tm_map(samp_clean, removeWords, profanity1) 

# Stem words and strip white space
samp_clean = tm_map(samp_clean, content_transformer(stemDocument)) 
samp_clean = tm_map(samp_clean, stripWhitespace) 

```

Begin the exploration.
First we check how many distinct words were caught by the sample and what are the most frequently occurring words in the sample.

```{r}
TDM = TermDocumentMatrix(samp_clean)
dim(TDM)
#inspect(TDM[1:10,1:3])

```

The dimensions of the matrix show that we have a term document matrix of 19896 distinct terms (stemmed!) from the 3 documents.

First we can look at the words occurring over 1000 times:
```{r}

findFreqTerms(TDM, 1000)

```

Calculate a term document matrix without the sparsely occuring words

```{r}
TDM.common = removeSparseTerms(TDM, 0.1)
TDM.common$dimnames$Docs = c("blogs", "news", "twitter")
#inspect(TDM.common[1:20,1:3])

```

Question 1: Some words are more frequent than others - what are the distributions of word frequencies? 

```{r}
# convert the matrix into a data table with a frequency of word in each sample
tdm_c = as.matrix(TDM.common)
tdm_c = as.data.frame.table(tdm_c)
tdm_c = as.data.table(tdm_c)
```

```{r}

library(ggplot2)

p <- ggplot(subset(tdm_c, Freq>500), aes(Terms, Freq))    
p <- p + geom_bar(stat="identity") + facet_wrap(~ Docs)   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p

#Source: https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html#loading-texts

```

Question 2. What are the frequencies of 2-grams and 3-grams in the dataset? 

```{r}

# Bigram and Trigram Tokenizer Functions

BigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 2, max = 2))

TrigramTokenizer <- function(x) NGramTokenizer(x, 
                                Weka_control(min = 3, max = 3))

tdm_bi <- TermDocumentMatrix(samp_clean, control = list(tokenize = TrigramTokenizer))

tdm_tri <- TermDocumentMatrix(samp_clean, control = list(tokenize = TrigramTokenizer))

#Source: http://stackoverflow.com/questions/19615181/finding-ngrams-in-r-and-comparing-ngrams-across-corpora

```


  
Generate a word cloud

```{r}
require(wordcloud)
wordcloud(samp_clean, max.words = 100, random.order = F, rot.per=0.0, colors=brewer.pal(3,"Dark2" ))

```


