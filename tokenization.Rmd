---
title: "Tokenization"
author: "Datafeellings"
date: "July 24, 2015"
output: html_document
---


Setup project workspace

```{r, cache=TRUE, echo=FALSE}

ifelse(!dir.exists(file.path("~/Coursera/Capstone/Source")),dir.create(file.path("~/Coursera/Capstone/Source")), FALSE)
ifelse(!dir.exists(file.path("~/Coursera/Capstone/Code")),dir.create(file.path("~/Coursera/Capstone/Code")), FALSE)
ifelse(!dir.exists(file.path("~/Coursera/Capstone/Figures")),dir.create(file.path("~/Coursera/Capstone/Figures")), FALSE)
ifelse(!dir.exists(file.path("~/Coursera/Capstone/Processed_data")),dir.create(file.path("~/Coursera/Capstone/Processed_data")), FALSE)
setwd("~/Coursera/Capstone")
```

Download the data

```{r}

# if (!file.exists("Coursera-SwiftKey.zip")) 
# {
#   download.file(url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile = "~/Documents/Disk D/Coursera/Capstone/Source/Coursera-SwiftKey.zip", method = "curl")
#   system("unzip Coursera-SwiftKey.zip")
# }


```

Load the data into the WS using readLines() function

```{r}

con1 <- file("~/Coursera/Capstone/Source/final/en_US/en_US.blogs.txt", "r")
en_b <- readLines(con1) 
close(con1)

con2 <- file("~/Coursera/Capstone/Source/final/en_US/en_US.news.txt", "r")
en_n <- readLines(con2) 
close(con2)

con3 <- file("~/Coursera/Capstone/Source/final/en_US/en_US.twitter.txt", "r")
en_t <- readLines(con3) 
close(con3)

```

Convert the read corpora into data tables and take a 3000 lines sample of each. Save the samples, as we will work with them for tokenization. For reproducibility, note the value of the set.seed() function.

```{r}

require(data.table) # load the required package
set.seed(888)

en_b = as.data.table(en_b) 
en_b_samp = en_b [sample(.N,3000)] # random sample of 3k lines
write.csv(en_b_samp, file = "Processed_data/en_b_samp.csv") #write the sample

en_n = as.data.table(en_n) 
en_n_samp = en_n [sample(.N,3000)]
write.csv(en_n_samp, file = "Processed_data/en_n_samp.csv")

en_t = as.data.table(en_t) 
en_t_samp = en_t[sample(.N,3000)]
write.csv(en_t_samp, file = "Processed_data/en_t_samp.csv")
```

Now we read in the samples using the tm package as a single corpus. As the goal is to tokenize the words and remove profanity, no differentiation is required between the different sources of the text.

```{r}
require("tm")
require("SnowballC")
samp = Corpus(DirSource("Processed_data"), readerControl = list(language = "en_EN", load = F))

```

Preprocessing the text using the automated functions of the "tm" package.

```{r}
# Input for this part for the workflow: https://gist.github.com/dsparks/4260167

# Make lowercase & remove punctuation
samp_clean = tm_map(samp, tolower)

# Remove special characters and punctuation

for(j in seq(samp_clean))   
{   
    samp_clean[[j]] <- gsub("/", " ", samp_clean[[j]])   
    samp_clean[[j]] <- gsub("@", " ", samp_clean[[j]])   
    samp_clean[[j]] <- gsub("\\|", " ", samp_clean[[j]])
    samp_clean[[j]] <- gsub("â€", " ", samp_clean[[j]])
}  
samp_clean = tm_map(samp_clean, PlainTextDocument)  
samp_clean = tm_map(samp_clean, removePunctuation)
samp_clean = tm_map(samp_clean, removeNumbers)

# Remove stopwords & stem words
samp_clean = tm_map(samp_clean, removeWords, stopwords("english")) 
samp_clean = tm_map(samp_clean, stemDocument) 

# Strip white space
samp_clean = tm_map(samp_clean, stripWhitespace) 

```

Removing profanity


```{r}
# source of the English language profanity list used by Google
# credit goes to https://gist.github.com/jamiew

con4 = url(description = "https://gist.githubusercontent.com/jamiew/1112488/raw/7ca9b1669e1c24b27c66174762cb04e14cf05aa7/google_twunter_lol")
profanity = readLines(con4)
close(con4)

dir.create(path = "Source/external")
write.csv(x = profanity, file = "Source/external/profanity.csv")
profanity1 = read.csv("Source/external/profanity.csv", skip = 1,nrows = 451)

# clean the file to turn it into a character string that can be used by tm 
profanity1=profanity1[,-1]
profanity1=as.character(profanity1)

profanity1=gsub("\"", x= profanity1, replacement ="")
profanity1=gsub(pattern =":1", x= profanity1, replacement ="")
profanity1=gsub(pattern =",", x= profanity1, replacement ="")

```



Generate a word cloud

```{r}
require(wordcloud)
wordcloud(samp_clean, max.words = 100, random.order = F, rot.per=0.0, colors=brewer.pal(3,"Dark2" ))

```

